{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c8efae",
   "metadata": {},
   "source": [
    "# Lab 6 - IR Inverted Matrix Workshop\n",
    "\n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "We collect a set of text documents to build our inverted index. The documents are from Free eBooks | Project Gutenberg (https://www.gutenberg.org/),which include various topics like crime, history, and more.\n",
    "\n",
    "### ðŸ”§ Our documents:\n",
    "- we collected 20 text documents into the `sample_docs folder`, from above source. \n",
    "- All has plain text and more than 2000 words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents('sample_docs/')\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f0cb2",
   "metadata": {},
   "source": [
    "This function read all documents from the `sample_docs` folder and returns the number documents loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "In this section we created a basic tokenizer to process the text documents. This tokenizer split each document into tokens (words) and removes punctuation. It also converts all tokens to lowercase and with a regular expression to remove any non-alphanumeric characters.\n",
    "\n",
    "At the end of this block, we will have a list of tokens for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'glimpses', 'of', 'the', 'dark', 'ages', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "Using `nltk` library, we will implement a normalization pipeline that includes stemming and stop word removal. This will help us reduce the vocabulary size and focus on the most relevant terms in our documents.\n",
    "\n",
    "For example, the word \"anyone\" will be stemmed to \"anyon\", and \"glimpse\" will be stemmed to \"glimps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'glimps', 'dark', 'age', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)  # Suppress download warnings\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ac4a5",
   "metadata": {},
   "source": [
    "## Previous token:\n",
    "\n",
    "`['the', 'project', 'gutenberg', 'ebook', 'of', 'glimpses', 'of', 'the', 'dark', 'ages', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']`\n",
    "\n",
    "## After remove stopwords and applying stemming:\n",
    "\n",
    "`['project', 'gutenberg', 'ebook', 'glimps', 'dark', 'age', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']`\n",
    "\n",
    "The difference between the previous and the new token is that the previous token contains stopwords such as \"the\", \"of\", \"is\", \"for\", \"in\", etc., while the new token has these words removed, leaving only the significant words that contribute to the meaning of the text.\n",
    "We saw that some letters were removed from the words, this is because we applied stemming to the words, which reduces them to their root form. For example, \"glimpses\" becomes \"glimps\", \"ages\" becomes \"age\", and \"anyone\" becomes \"anyon\". This helps in reducing the vocabulary size and improving search accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "In this step, we are creating an inverted index from the processed tokens. The inverted index maps each unique token to the list of documents (or their IDs) where that token appears. This is a crucial step in building an efficient search engine.\n",
    "\n",
    "One example is: \n",
    "\n",
    "`'glimps': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'gutenberg': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'ebook': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'glimps': [0, 3, 6, 7, 8, 10, 12, 14, 15], 'dark': [0, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16], 'age': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'use': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'anyon': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'anywher': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'unit': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "Here, we are implementing queries using positional indexing. This allows us to search for phrases within the documents.\n",
    "\n",
    "To support **exact phrase search**, we store the position of each word in each document. The search function only returns documents where the full sequence of words appears **in order and without gaps**.\n",
    " \n",
    "We test two phrases:\n",
    "- `\"crime and punishment\"`\n",
    "- `\"this ebook\"`\n",
    " \n",
    "The function will print document indices and a short preview from each result for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c60c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Results for 'crime and punishment': [3, 16]\n",
      "â†’ Doc 3 preview: ï»¿The Project Gutenberg eBook of Crime and Punishment\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of th...\n",
      "â†’ Doc 16 preview: ï»¿The Project Gutenberg eBook of Voyage to the East Indies\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "ðŸ”Ž Results for 'this ebook': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "â†’ Doc 0 preview: ï»¿The Project Gutenberg eBook of Glimpses of the Dark Ages\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "â†’ Doc 1 preview: ï»¿The Project Gutenberg eBook of Hannele\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no...\n",
      "â†’ Doc 2 preview: ï»¿The Project Gutenberg eBook of A history of the Peninsular War, Vol. 3, Sep. 1809-Dec. 1810\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the ...\n",
      "â†’ Doc 3 preview: ï»¿The Project Gutenberg eBook of Crime and Punishment\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of th...\n",
      "â†’ Doc 4 preview: ï»¿The Project Gutenberg eBook of The Eyes Have It\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the wo...\n",
      "â†’ Doc 5 preview: ï»¿The Project Gutenberg eBook of A Doll's House : a play\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of...\n",
      "â†’ Doc 6 preview: ï»¿The Project Gutenberg eBook of Hamlet, Prince of Denmark\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "â†’ Doc 7 preview: ï»¿The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts o...\n",
      "â†’ Doc 8 preview: ï»¿The Project Gutenberg eBook of Cambridge Sketches\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the ...\n",
      "â†’ Doc 9 preview: ï»¿The Project Gutenberg eBook of The Sceptical Chymist\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of t...\n",
      "â†’ Doc 10 preview: ï»¿The Project Gutenberg eBook of Right Ho, Jeeves\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the wo...\n",
      "â†’ Doc 11 preview: ï»¿The Project Gutenberg eBook of Ã‰crits spirituels de Charles de Foucauld\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "mo...\n",
      "â†’ Doc 12 preview: ï»¿The Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no...\n",
      "â†’ Doc 13 preview: ï»¿The Project Gutenberg eBook of The dialogues of Plato in five volumes, Vol. II (of 5)\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United...\n",
      "â†’ Doc 14 preview: ï»¿The Project Gutenberg eBook of Bog-trotting for orchids\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts o...\n",
      "â†’ Doc 15 preview: ï»¿The Project Gutenberg eBook of The adventures of Heine\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of...\n",
      "â†’ Doc 16 preview: ï»¿The Project Gutenberg eBook of Voyage to the East Indies\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "â†’ Doc 17 preview: ï»¿The Project Gutenberg eBook of The art of fiction\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the ...\n",
      "â†’ Doc 18 preview: ï»¿The Project Gutenberg eBook of OrÃ­genes de la novela - Vol. III\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other...\n",
      "â†’ Doc 19 preview: ï»¿The Project Gutenberg eBook of Juomalakko\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at...\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# --- Positional Index for Phrase Queries ---\n",
    "def build_positional_index(documents):\n",
    "    positional_index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        for pos, token in enumerate(tokens):\n",
    "            positional_index[token][doc_id].append(pos)\n",
    "    return positional_index\n",
    "\n",
    "# --- Phrase Query Search Function ---\n",
    "def phrase_search(query, positional_index):\n",
    "    words = normalize_tokens(tokenize(query))\n",
    "    if not words:\n",
    "        return []\n",
    "    doc_sets = [set(positional_index[word].keys()) for word in words if word in positional_index]\n",
    "    if len(doc_sets) != len(words):\n",
    "        return []\n",
    "    candidate_docs = set.intersection(*doc_sets)\n",
    "    matching_docs = []\n",
    "    for doc_id in candidate_docs:\n",
    "        positions = [positional_index[word][doc_id] for word in words]\n",
    "        for pos in positions[0]:\n",
    "            if all((pos + i) in positions[i] for i in range(1, len(words))):\n",
    "                matching_docs.append(doc_id)\n",
    "                break\n",
    "    return matching_docs\n",
    "\n",
    "# --- Build Positional Index ---\n",
    "positional_index = build_positional_index(documents)\n",
    "\n",
    "# --- Test Phrase Queries ---\n",
    "query1 = \"crime and punishment\"\n",
    "query2 = \"this ebook\"\n",
    "\n",
    "results1 = phrase_search(query1, positional_index)\n",
    "results2 = phrase_search(query2, positional_index)\n",
    "\n",
    "print(f\"ðŸ”Ž Results for '{query1}':\", results1)\n",
    "for i in results1:\n",
    "    print(f'â†’ Doc {i} preview: {documents[i][:150]}...')\n",
    "\n",
    "print(f\"ðŸ”Ž Results for '{query2}':\", results2)\n",
    "for i in results2:\n",
    "    print(f'â†’ Doc {i} preview: {documents[i][:150]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea540da",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Talking Point Number 1:\n",
    ">  Flow of the workshop:\n",
    "Collect documents -> Tokenize -> Normalize -> Build Inverted Index -> Phrase Queries\n",
    "\n",
    "By applying tokenization and normalization (including lowercasing, stopword removal, and optional stemming), we ensured that the index remains compact and consistent, improving both storage efficiency and search accuracy.\n",
    "\n",
    "Building the inverted index allows us to quickly locate documents containing specific terms, which is essential for efficient search operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c78e4b",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Talking Point Number 2:\n",
    ">  Phrase Query Search with Positional Index and differences between inverted index and positional index\n",
    "\n",
    "We implemented a phrase query search using positional indexing with Pythonâ€™s defaultdict. \n",
    "\n",
    "- The build_positional_index() function records the positions of each word in every document after tokenization and normalization.\n",
    "\n",
    "- The phrase_search() function then checks these positions to find documents where all words in a phrase appear together and in order. This approach allows us to accurately support exact phrase queries, such as \"crime and punishment\", rather than just matching individual keywords.\n",
    "\n",
    "The main difference between inverted index and positional index is an inverted index maps each word to the documents it appears in, while a positional index also records the exact positions of each word in the documents, enabling precise phrase searches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
