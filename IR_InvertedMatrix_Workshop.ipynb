{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents('sample_docs/')\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "\n",
    "# Example:\n",
    "query1 = \"machine learning\"\n",
    "query2 = \"deep model\"\n",
    "\n",
    "# Your implementation here\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
