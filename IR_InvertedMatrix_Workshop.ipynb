{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c8efae",
   "metadata": {},
   "source": [
    "# Lab 6 - IR Inverted Matrix Workshop\n",
    "\n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "We collect a set of text documents to build our inverted index. The documents are from Free eBooks | Project Gutenberg (https://www.gutenberg.org/),which include various topics like crime, history, and more.\n",
    "\n",
    "### üîß Our documents:\n",
    "- we collected 20 text documents into the `sample_docs folder`, from above source. \n",
    "- All has plain text and more than 2000 words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents('sample_docs/')\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f0cb2",
   "metadata": {},
   "source": [
    "This function read all documents from the `sample_docs` folder and returns the number documents loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "In this section we created a basic tokenizer to process the text documents. This tokenizer split each document into tokens (words) and removes punctuation. It also converts all tokens to lowercase and with a regular expression to remove any non-alphanumeric characters.\n",
    "\n",
    "At the end of this block, we will have a list of tokens for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'glimpses', 'of', 'the', 'dark', 'ages', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## üîÅ Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "Using `nltk` library, we will implement a normalization pipeline that includes stemming and stop word removal. This will help us reduce the vocabulary size and focus on the most relevant terms in our documents.\n",
    "\n",
    "For example, the word \"anyone\" will be stemmed to \"anyon\", and \"glimpse\" will be stemmed to \"glimps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'glimps', 'dark', 'age', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)  # Suppress download warnings\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ac4a5",
   "metadata": {},
   "source": [
    "## Previous token:\n",
    "\n",
    "`['the', 'project', 'gutenberg', 'ebook', 'of', 'glimpses', 'of', 'the', 'dark', 'ages', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']`\n",
    "\n",
    "## After remove stopwords and applying stemming:\n",
    "\n",
    "`['project', 'gutenberg', 'ebook', 'glimps', 'dark', 'age', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']`\n",
    "\n",
    "The difference between the previous and the new token is that the previous token contains stopwords such as \"the\", \"of\", \"is\", \"for\", \"in\", etc., while the new token has these words removed, leaving only the significant words that contribute to the meaning of the text.\n",
    "We saw that some letters were removed from the words, this is because we applied stemming to the words, which reduces them to their root form. For example, \"glimpses\" becomes \"glimps\", \"ages\" becomes \"age\", and \"anyone\" becomes \"anyon\". This helps in reducing the vocabulary size and improving search accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## üîç Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "In this step, we are creating an inverted index from the processed tokens. The inverted index maps each unique token to the list of documents (or their IDs) where that token appears. This is a crucial step in building an efficient search engine.\n",
    "\n",
    "One example is: \n",
    "\n",
    "`'glimps': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'gutenberg': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'ebook': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'glimps': [0, 3, 6, 7, 8, 10, 12, 14, 15], 'dark': [0, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16], 'age': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'use': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'anyon': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'anywher': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'unit': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## üß™ Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "Here, we are implementing queries using positional indexing. This allows us to search for phrases within the documents.\n",
    "\n",
    "To support **exact phrase search**, we store the position of each word in each document. The search function only returns documents where the full sequence of words appears **in order and without gaps**.\n",
    " \n",
    "We test two phrases:\n",
    "- `\"crime and punishment\"`\n",
    "- `\"this ebook\"`\n",
    " \n",
    "The function will print document indices and a short preview from each result for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c60c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Results for 'crime and punishment': [3, 16]\n",
      "‚Üí Doc 3 preview: ÔªøThe Project Gutenberg eBook of Crime and Punishment\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of th...\n",
      "‚Üí Doc 16 preview: ÔªøThe Project Gutenberg eBook of Voyage to the East Indies\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "üîé Results for 'this ebook': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "‚Üí Doc 0 preview: ÔªøThe Project Gutenberg eBook of Glimpses of the Dark Ages\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "‚Üí Doc 1 preview: ÔªøThe Project Gutenberg eBook of Hannele\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no...\n",
      "‚Üí Doc 2 preview: ÔªøThe Project Gutenberg eBook of A history of the Peninsular War, Vol. 3, Sep. 1809-Dec. 1810\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the ...\n",
      "‚Üí Doc 3 preview: ÔªøThe Project Gutenberg eBook of Crime and Punishment\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of th...\n",
      "‚Üí Doc 4 preview: ÔªøThe Project Gutenberg eBook of The Eyes Have It\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the wo...\n",
      "‚Üí Doc 5 preview: ÔªøThe Project Gutenberg eBook of A Doll's House : a play\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of...\n",
      "‚Üí Doc 6 preview: ÔªøThe Project Gutenberg eBook of Hamlet, Prince of Denmark\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "‚Üí Doc 7 preview: ÔªøThe Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts o...\n",
      "‚Üí Doc 8 preview: ÔªøThe Project Gutenberg eBook of Cambridge Sketches\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the ...\n",
      "‚Üí Doc 9 preview: ÔªøThe Project Gutenberg eBook of The Sceptical Chymist\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of t...\n",
      "‚Üí Doc 10 preview: ÔªøThe Project Gutenberg eBook of Right Ho, Jeeves\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the wo...\n",
      "‚Üí Doc 11 preview: ÔªøThe Project Gutenberg eBook of √âcrits spirituels de Charles de Foucauld\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "mo...\n",
      "‚Üí Doc 12 preview: ÔªøThe Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no...\n",
      "‚Üí Doc 13 preview: ÔªøThe Project Gutenberg eBook of The dialogues of Plato in five volumes, Vol. II (of 5)\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United...\n",
      "‚Üí Doc 14 preview: ÔªøThe Project Gutenberg eBook of Bog-trotting for orchids\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts o...\n",
      "‚Üí Doc 15 preview: ÔªøThe Project Gutenberg eBook of The adventures of Heine\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of...\n",
      "‚Üí Doc 16 preview: ÔªøThe Project Gutenberg eBook of Voyage to the East Indies\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts ...\n",
      "‚Üí Doc 17 preview: ÔªøThe Project Gutenberg eBook of The art of fiction\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the ...\n",
      "‚Üí Doc 18 preview: ÔªøThe Project Gutenberg eBook of Or√≠genes de la novela - Vol. III\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other...\n",
      "‚Üí Doc 19 preview: ÔªøThe Project Gutenberg eBook of Juomalakko\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at...\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# --- Positional Index for Phrase Queries ---\n",
    "def build_positional_index(documents):\n",
    "    positional_index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        for pos, token in enumerate(tokens):\n",
    "            positional_index[token][doc_id].append(pos)\n",
    "    return positional_index\n",
    "\n",
    "# --- Phrase Query Search Function ---\n",
    "def phrase_search(query, positional_index):\n",
    "    words = normalize_tokens(tokenize(query))\n",
    "    if not words:\n",
    "        return []\n",
    "    doc_sets = [set(positional_index[word].keys()) for word in words if word in positional_index]\n",
    "    if len(doc_sets) != len(words):\n",
    "        return []\n",
    "    candidate_docs = set.intersection(*doc_sets)\n",
    "    matching_docs = []\n",
    "    for doc_id in candidate_docs:\n",
    "        positions = [positional_index[word][doc_id] for word in words]\n",
    "        for pos in positions[0]:\n",
    "            if all((pos + i) in positions[i] for i in range(1, len(words))):\n",
    "                matching_docs.append(doc_id)\n",
    "                break\n",
    "    return matching_docs\n",
    "\n",
    "# --- Build Positional Index ---\n",
    "positional_index = build_positional_index(documents)\n",
    "\n",
    "# --- Test Phrase Queries ---\n",
    "query1 = \"crime and punishment\"\n",
    "query2 = \"this ebook\"\n",
    "\n",
    "results1 = phrase_search(query1, positional_index)\n",
    "results2 = phrase_search(query2, positional_index)\n",
    "\n",
    "print(f\"üîé Results for '{query1}':\", results1)\n",
    "for i in results1:\n",
    "    print(f'‚Üí Doc {i} preview: {documents[i][:150]}...')\n",
    "\n",
    "print(f\"üîé Results for '{query2}':\", results2)\n",
    "for i in results2:\n",
    "    print(f'‚Üí Doc {i} preview: {documents[i][:150]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea540da",
   "metadata": {},
   "source": [
    "\n",
    "### üó£ Talking Point Number 1:\n",
    "Flow of the workshop:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
